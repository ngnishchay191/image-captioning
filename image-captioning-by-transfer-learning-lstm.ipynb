{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Import Necessary Libraries\nfrom os import listdir\nfrom pickle import dump\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model","metadata":{"id":"EeWV4MEMvvMd","execution":{"iopub.status.busy":"2021-12-05T18:46:28.193517Z","iopub.execute_input":"2021-12-05T18:46:28.194165Z","iopub.status.idle":"2021-12-05T18:46:32.890851Z","shell.execute_reply.started":"2021-12-05T18:46:28.194060Z","shell.execute_reply":"2021-12-05T18:46:32.890012Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# extract features from each photo in the directory\ndef extract_features(directory):\n\t# load the model\n\tmodel = VGG16()\n\t# re-structure the model\n\tmodel.layers.pop()\n\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n\t# summarize\n\tprint(model.summary())\n\t# extract features from each photo\n\tfeatures = dict()\n\tfor name in listdir(directory):\n\t\t# load an image from file\n\t\tfilename = directory + '/' + name\n\t\timage = load_img(filename, target_size=(224, 224))\n\t\t# convert the image pixels to a numpy array\n\t\timage = img_to_array(image)\n\t\t# reshape data for the model\n\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n\t\t# prepare the image for the VGG model\n\t\timage = preprocess_input(image)\n\t\t# get features\n\t\tfeature = model.predict(image, verbose=0)\n\t\t# get image id\n\t\timage_id = name.split('.')[0]\n\t\t# store feature\n\t\tfeatures[image_id] = feature\n\t\t# print('>%s' % name)\n\treturn features\n\n# extract features from all images\ndirectory = '../input/flickr8k/Images'\nfeatures = extract_features(directory)\nprint('Extracted Features: %d' % len(features))\n# save to file\ndump(features, open('features.pkl', 'wb'))\n# features['3392851587_a638ff25e2'].shape","metadata":{"id":"UYgL4SLBSYCN","outputId":"40ca3c63-3632-45f6-a503-bef6bfc36f7d","execution":{"iopub.status.busy":"2021-12-05T18:46:32.892702Z","iopub.execute_input":"2021-12-05T18:46:32.892992Z","iopub.status.idle":"2021-12-05T18:54:23.978232Z","shell.execute_reply.started":"2021-12-05T18:46:32.892955Z","shell.execute_reply":"2021-12-05T18:54:23.977244Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import string\n\n# load doc into memory\ndef load_doc(filename):\n\t\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\n\n# extract descriptions for images\ndef load_descriptions(doc):\n\tmapping = dict()\n\t# process lines\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\tif len(line) < 2:\n\t\t\tcontinue\n\t\t# take the first token as the image id, the rest as the description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# remove filename from image id\n\t\timage_id = image_id.split('.')[0]\n\t\t# convert description tokens back to string\n\t\timage_desc = ' '.join(image_desc)\n\t\t# create the list if needed\n\t\tif image_id not in mapping:\n\t\t\tmapping[image_id] = list()\n\t\t# store description\n\t\tmapping[image_id].append(image_desc)\n\treturn mapping\n\ndef clean_descriptions(descriptions):\n\t# prepare translation table for removing punctuation\n\ttable = str.maketrans('', '', string.punctuation)\n\tfor key, desc_list in descriptions.items():\n\t\tfor i in range(len(desc_list)):\n\t\t\tdesc = desc_list[i]\n\t\t\t# tokenize\n\t\t\tdesc = desc.split()\n\t\t\t# convert to lower case\n\t\t\tdesc = [word.lower() for word in desc]\n\t\t\t# remove punctuation from each token\n\t\t\tdesc = [w.translate(table) for w in desc]\n\t\t\t# remove hanging 's' and 'a'\n\t\t\tdesc = [word for word in desc if len(word)>1]\n\t\t\t# remove tokens with numbers in them\n\t\t\tdesc = [word for word in desc if word.isalpha()]\n\t\t\t# store as string\n\t\t\tdesc_list[i] =  ' '.join(desc)\n\n# convert the loaded descriptions into a vocabulary of words\ndef to_vocabulary(descriptions):\n\t# build a list of all description strings\n\tall_desc = set()\n\tfor key in descriptions.keys():\n\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n\treturn all_desc\n\n# save descriptions to file, one per line\ndef save_descriptions(descriptions, filename):\n\tlines = list()\n\tfor key, desc_list in descriptions.items():\n\t\tfor desc in desc_list:\n\t\t\tlines.append(key + ' ' + desc)\n\tdata = '\\n'.join(lines)\n\tfile = open(filename, 'w')\n\tfile.write(data)\n\tfile.close()\n\n# filename = 'Flickr8k_text/Flickr8k.token.txt'\nfilename = '../input/flickr8k-text/Flickr8k.token.txt'\n# load descriptions\ndoc = load_doc(filename)\n# parse descriptions\ndescriptions = load_descriptions(doc)\nprint('Loaded: %d ' % len(descriptions))\n# clean descriptions\nclean_descriptions(descriptions)\n# summarize vocabulary\nvocabulary = to_vocabulary(descriptions)\nprint('Vocabulary Size: %d' % len(vocabulary))\n# save to file\nsave_descriptions(descriptions, 'descriptions.txt')","metadata":{"id":"rnHPW0WTwJqH","outputId":"44ab8af5-8915-48e4-e06a-3cf731bb8217","execution":{"iopub.status.busy":"2021-12-05T18:54:23.982009Z","iopub.execute_input":"2021-12-05T18:54:23.982314Z","iopub.status.idle":"2021-12-05T18:54:24.961147Z","shell.execute_reply.started":"2021-12-05T18:54:23.982285Z","shell.execute_reply":"2021-12-05T18:54:24.960365Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from pickle import load\n\n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\n\n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n\tdoc = load_doc(filename)\n\tdataset = list()\n\t# process line by line\n\tfor line in doc.split('\\n'):\n\t\t# skip empty lines\n\t\tif len(line) < 1:\n\t\t\tcontinue\n\t\t# get the image identifier\n\t\tidentifier = line.split('.')[0]\n\t\tdataset.append(identifier)\n\treturn set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n\t# load document\n\tdoc = load_doc(filename)\n\tdescriptions = dict()\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\t# split id from description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# skip images not in the set\n\t\tif image_id in dataset:\n\t\t\t# create list\n\t\t\tif image_id not in descriptions:\n\t\t\t\tdescriptions[image_id] = list()\n\t\t\t# wrap description in tokens\n\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n\t\t\t# store\n\t\t\tdescriptions[image_id].append(desc)\n\treturn descriptions\n\n# load photo features\ndef load_photo_features(filename, dataset):\n\t# load all features\n\tall_features = load(open(filename, 'rb'))\n\t# filter features\n\tfeatures = {k: all_features[k] for k in dataset}\n\treturn features\n\n# load training dataset (6K)\nfilename = '../input/flickr8k-text/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# photo features\ntrain_features = load_photo_features('features.pkl', train)\nprint('Photos: train=%d' % len(train_features))\ntrain_descriptions\n","metadata":{"id":"ynKboVDMwylP","outputId":"dd8c4d1e-a8b4-41ae-d79c-e53905043c57","execution":{"iopub.status.busy":"2021-12-05T18:54:24.962678Z","iopub.execute_input":"2021-12-05T18:54:24.963172Z","iopub.status.idle":"2021-12-05T18:54:25.325798Z","shell.execute_reply.started":"2021-12-05T18:54:24.963110Z","shell.execute_reply":"2021-12-05T18:54:25.325048Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install keras.utils ","metadata":{"id":"DuaqY-wIeW4L","outputId":"6da6e05c-311e-4a27-b2b5-76be8b627e66","execution":{"iopub.status.busy":"2021-12-05T18:54:25.327964Z","iopub.execute_input":"2021-12-05T18:54:25.328768Z","iopub.status.idle":"2021-12-05T18:54:33.570444Z","shell.execute_reply.started":"2021-12-05T18:54:25.328725Z","shell.execute_reply":"2021-12-05T18:54:33.569582Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import tensorflow \nimport tensorflow.keras as keras","metadata":{"id":"mfcEVSJ2fNad","execution":{"iopub.status.busy":"2021-12-05T18:54:33.573053Z","iopub.execute_input":"2021-12-05T18:54:33.573349Z","iopub.status.idle":"2021-12-05T18:54:33.715884Z","shell.execute_reply.started":"2021-12-05T18:54:33.573310Z","shell.execute_reply":"2021-12-05T18:54:33.715082Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"id":"rl6wCew3ev08","outputId":"a8c14439-9fee-40ce-8785-1d452b18d4d2","execution":{"iopub.status.busy":"2021-12-05T18:54:33.717159Z","iopub.execute_input":"2021-12-05T18:54:33.717517Z","iopub.status.idle":"2021-12-05T18:54:41.324663Z","shell.execute_reply.started":"2021-12-05T18:54:33.717478Z","shell.execute_reply":"2021-12-05T18:54:41.323757Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from numpy import array\n\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n# from keras.utils import to_categorical\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n\n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\n\n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n\tdoc = load_doc(filename)\n\tdataset = list()\n\t# process line by line\n\tfor line in doc.split('\\n'):\n\t\t# skip empty lines\n\t\tif len(line) < 1:\n\t\t\tcontinue\n\t\t# get the image identifier\n\t\tidentifier = line.split('.')[0]\n\t\tdataset.append(identifier)\n\treturn set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n\t# load document\n\tdoc = load_doc(filename)\n\tdescriptions = dict()\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\t# split id from description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# skip images not in the set\n\t\tif image_id in dataset:\n\t\t\t# create list\n\t\t\tif image_id not in descriptions:\n\t\t\t\tdescriptions[image_id] = list()\n\t\t\t# wrap description in tokens\n\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n\t\t\t# store\n\t\t\tdescriptions[image_id].append(desc)\n\treturn descriptions\n\n# load photo features\ndef load_photo_features(filename, dataset):\n\t# load all features\n\tall_features = load(open(filename, 'rb'))\n\t# filter features\n\tfeatures = {k: all_features[k] for k in dataset}\n\treturn features\n\n# covert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n\tall_desc = list()\n\tfor key in descriptions.keys():\n\t\t[all_desc.append(d) for d in descriptions[key]]\n\treturn all_desc\n\n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n\tlines = to_lines(descriptions)\n\ttokenizer = Tokenizer()\n\ttokenizer.fit_on_texts(lines)\n\treturn tokenizer\n\n# get vocabulary size\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n\n# calculate the length of the description with the most words\ndef max_length(descriptions):\n\tlines = to_lines(descriptions)\n\treturn max(len(d.split()) for d in lines)\n\n# create sequences of images, input sequences and output words for an image\ndef create_sequences(tokenizer, max_length, desc_list, photo):\n\tX1, X2, y = list(), list(), list()\n\t# walk through each description for the image\n\tfor desc in desc_list:\n\t\t# encode the sequence\n\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n\t\t# split one sequence into multiple X,y pairs\n\t\tfor i in range(1, len(seq)):\n\t\t\t# split into input and output pair\n\t\t\tin_seq, out_seq = seq[:i], seq[i]\n\t\t\t# pad input sequence\n\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n\t\t\t# encode output sequence\n\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n\t\t\t# store\n\t\t\tX1.append(photo)\n\t\t\tX2.append(in_seq)\n\t\t\ty.append(out_seq)\n\treturn array(X1), array(X2), array(y)\n\n# define the captioning model\ndef define_model(vocab_size, max_length):\n\t# feature extractor model\n\tinputs1 = Input(shape=(4096,))\n\tfe1 = Dropout(0.5)(inputs1)\n\tfe2 = Dense(256, activation='relu')(fe1)\n\t# sequence model\n\tinputs2 = Input(shape=(max_length,))\n\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n\tse2 = Dropout(0.5)(se1)\n\tse3 = LSTM(256)(se2)\n\t# decoder model\n\tdecoder1 = Concatenate()([fe2, se3])\n\tdecoder2 = Dense(512, activation='relu')(decoder1)\n\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\t# tie it together [image, seq] [word]\n\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\t# summarize model\n\tprint(model.summary())\n\treturn model\n","metadata":{"id":"Nie7mc0rxMne","outputId":"978afb3f-ba97-4f7b-aeb5-355dbf162b0c","execution":{"iopub.status.busy":"2021-12-05T18:54:41.326821Z","iopub.execute_input":"2021-12-05T18:54:41.327099Z","iopub.status.idle":"2021-12-05T18:54:41.807194Z","shell.execute_reply.started":"2021-12-05T18:54:41.327062Z","shell.execute_reply":"2021-12-05T18:54:41.806291Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Below code is used to progressively load the batch of data\n# data generator, intended to be used in a call to model.fit_generator()\ndef data_generator(descriptions, photos, tokenizer, max_length):\n\t# loop for ever over images\n\twhile 1:\n\t\tfor key, desc_list in descriptions.items():\n\t\t\t# retrieve the photo feature\n\t\t\tphoto = photos[key][0]\n\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n\t\t\tyield ([in_img, in_seq], out_word)","metadata":{"id":"aXsPgJiUPvYr","execution":{"iopub.status.busy":"2021-12-05T18:54:41.808818Z","iopub.execute_input":"2021-12-05T18:54:41.809100Z","iopub.status.idle":"2021-12-05T18:54:41.814432Z","shell.execute_reply.started":"2021-12-05T18:54:41.809062Z","shell.execute_reply":"2021-12-05T18:54:41.813713Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# load training dataset (6K)\nfilename = '../input/flickr8k-text/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# photo features\ntrain_features = load_photo_features('features.pkl', train)\nprint('Photos: train=%d' % len(train_features))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n# determine the maximum sequence length\nmax_length = max_length(train_descriptions)\nprint('Description Length: %d' % max_length)\n","metadata":{"id":"6e-yqx26J4om","outputId":"890b50e9-f865-48d0-b7e2-6787bbcef5d3","execution":{"iopub.status.busy":"2021-12-05T18:54:41.816060Z","iopub.execute_input":"2021-12-05T18:54:41.816568Z","iopub.status.idle":"2021-12-05T18:54:42.552214Z","shell.execute_reply.started":"2021-12-05T18:54:41.816529Z","shell.execute_reply":"2021-12-05T18:54:42.551377Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_features['1118557877_736f339752'].shape","metadata":{"id":"ea7OD64ov1ma","outputId":"8d1e1081-2115-4fd6-a5a3-7d014097eff9","execution":{"iopub.status.busy":"2021-12-05T18:54:42.553591Z","iopub.execute_input":"2021-12-05T18:54:42.553867Z","iopub.status.idle":"2021-12-05T18:54:42.559576Z","shell.execute_reply.started":"2021-12-05T18:54:42.553831Z","shell.execute_reply":"2021-12-05T18:54:42.558881Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# train the model\nmodel = define_model(vocab_size, max_length)\n# train the model, run epochs manually and save after each epoch\nepochs = 20\nsteps = len(train_descriptions)\nfor i in range(epochs):\n\t# create the data generator\n\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n\t# fit for one epoch\n\tmodel.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n\t# save model\n\tmodel.save('model_' + str(i) + '.h5')","metadata":{"id":"gRKJ6Ao0NO6L","outputId":"dcb5ef48-9283-433a-ada5-8f68f192e002","execution":{"iopub.status.busy":"2021-12-05T18:54:42.560955Z","iopub.execute_input":"2021-12-05T18:54:42.561354Z","iopub.status.idle":"2021-12-05T22:20:02.113575Z","shell.execute_reply.started":"2021-12-05T18:54:42.561317Z","shell.execute_reply":"2021-12-05T22:20:02.112753Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Entire code with all the functions, for reference purpose\n\nfrom numpy import argmax\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\n\n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n\tdoc = load_doc(filename)\n\tdataset = list()\n\t# process line by line\n\tfor line in doc.split('\\n'):\n\t\t# skip empty lines\n\t\tif len(line) < 1:\n\t\t\tcontinue\n\t\t# get the image identifier\n\t\tidentifier = line.split('.')[0]\n\t\tdataset.append(identifier)\n\treturn set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n\t# load document\n\tdoc = load_doc(filename)\n\tdescriptions = dict()\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\t# split id from description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# skip images not in the set\n\t\tif image_id in dataset:\n\t\t\t# create list\n\t\t\tif image_id not in descriptions:\n\t\t\t\tdescriptions[image_id] = list()\n\t\t\t# wrap description in tokens\n\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n\t\t\t# store\n\t\t\tdescriptions[image_id].append(desc)\n\treturn descriptions\n\n# load photo features\ndef load_photo_features(filename, dataset):\n\t# load all features\n\tall_features = load(open(filename, 'rb'))\n\t# filter features\n\tfeatures = {k: all_features[k] for k in dataset}\n\treturn features\n\n# covert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n\tall_desc = list()\n\tfor key in descriptions.keys():\n\t\t[all_desc.append(d) for d in descriptions[key]]\n\treturn all_desc\n\n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n\tlines = to_lines(descriptions)\n\ttokenizer = Tokenizer()\n\ttokenizer.fit_on_texts(lines)\n\treturn tokenizer\n\n# calculate the length of the description with the most words\ndef max_length(descriptions):\n\tlines = to_lines(descriptions)\n\treturn max(len(d.split()) for d in lines)\n\n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == integer:\n\t\t\treturn word\n\treturn None\n\n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n\t# seed the generation process\n\tin_text = 'startseq'\n\t# iterate over the whole length of the sequence\n\tfor i in range(max_length):\n\t\t# integer encode input sequence\n\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n\t\t# pad input\n\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n\t\t# predict next word\n\t\tyhat = model.predict([photo,sequence], verbose=0)\n\t\t# convert probability to integer\n\t\tyhat = argmax(yhat)\n\t\t# map integer to word\n\t\tword = word_for_id(yhat, tokenizer)\n\t\t# stop if we cannot map the word\n\t\tif word is None:\n\t\t\tbreak\n\t\t# append as input for generating the next word\n\t\tin_text += ' ' + word\n\t\t# stop if we predict the end of the sequence\n\t\tif word == 'endseq':\n\t\t\tbreak\n\treturn in_text\n\n# evaluate the skill of the model\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\n\tactual, predicted = list(), list()\n\t# step over the whole set\n\tfor key, desc_list in descriptions.items():\n\t\t# generate description\n\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n\t\t# store actual and predicted\n\t\treferences = [d.split() for d in desc_list]\n\t\tactual.append(references)\n\t\tpredicted.append(yhat.split())\n\t# calculate BLEU score\n\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n\n# prepare training set\n\n# load training dataset (6K)\nfilename = '../input/flickr8k-text/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n# determine the maximum sequence length\nmax_length = max_length(train_descriptions)\nprint('Description Length: %d' % max_length)\n\n# prepare test set\n\n# load test set\nfilename = '../input/flickr8k-text/Flickr_8k.testImages.txt'\ntest = load_set(filename)\nprint('Dataset: %d' % len(test))\n# descriptions\ntest_descriptions = load_clean_descriptions('descriptions.txt', test)\nprint('Descriptions: test=%d' % len(test_descriptions))\n# photo features\ntest_features = load_photo_features('features.pkl', test)\nprint('Photos: test=%d' % len(test_features))\n\n# load the model which has minimum loss, in this case it was model_18\nfilename = 'model_19.h5'\nmodel = load_model(filename)\n# evaluate model\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"id":"bUxkQAji4SkC","execution":{"iopub.status.busy":"2021-12-05T23:06:53.384207Z","iopub.execute_input":"2021-12-05T23:06:53.384511Z","iopub.status.idle":"2021-12-05T23:14:01.744798Z","shell.execute_reply.started":"2021-12-05T23:06:53.384478Z","shell.execute_reply":"2021-12-05T23:14:01.743411Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2021-12-05T22:33:02.360278Z","iopub.execute_input":"2021-12-05T22:33:02.360548Z","iopub.status.idle":"2021-12-05T22:33:02.364718Z","shell.execute_reply.started":"2021-12-05T22:33:02.360518Z","shell.execute_reply":"2021-12-05T22:33:02.363809Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# from keras.utils import plot_model\nplot_model(model, to_file='model.png')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T22:34:27.512654Z","iopub.execute_input":"2021-12-05T22:34:27.513484Z","iopub.status.idle":"2021-12-05T22:34:28.402512Z","shell.execute_reply.started":"2021-12-05T22:34:27.513435Z","shell.execute_reply":"2021-12-05T22:34:28.401600Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('tokenizer.pkl', 'wb') as handle:\n    dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T22:35:10.447309Z","iopub.execute_input":"2021-12-05T22:35:10.448007Z","iopub.status.idle":"2021-12-05T22:35:10.462515Z","shell.execute_reply.started":"2021-12-05T22:35:10.447963Z","shell.execute_reply":"2021-12-05T22:35:10.461575Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#Generate Captions for a Fresh Image\n\nfrom pickle import load\nfrom numpy import argmax\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nfrom keras.models import load_model\n\n# extract features from each photo in the directory\ndef extract_features(filename):\n\t# load the model\n\tmodel = VGG16()\n\t# re-structure the model\n\tmodel.layers.pop()\n\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n\t# load the photo\n\timage = load_img(filename, target_size=(224, 224))\n\t# convert the image pixels to a numpy array\n\timage = img_to_array(image)\n\t# reshape data for the model\n\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n\t# prepare the image for the VGG model\n\timage = preprocess_input(image)\n\t# get features\n\tfeature = model.predict(image, verbose=0)\n\treturn feature\n\n# load the tokenizer\ntokenizer = load(open('tokenizer.pkl', 'rb'))\n# pre-define the max sequence length (from training)\nmax_length = 34\n# load the model\nmodel = load_model('model_19.h5')\n# load and prepare the photograph\nphoto = extract_features('../input/test-photos/harshit.jpg')\n# generate description\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(description)","metadata":{"id":"MhshDtcS5iGi","execution":{"iopub.status.busy":"2021-12-05T23:15:29.703222Z","iopub.execute_input":"2021-12-05T23:15:29.703724Z","iopub.status.idle":"2021-12-05T23:15:34.234179Z","shell.execute_reply.started":"2021-12-05T23:15:29.703667Z","shell.execute_reply":"2021-12-05T23:15:34.233373Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#Remove startseq and endseq\nquery = description\nstopwords = ['startseq','endseq']\nquerywords = query.split()\n\nresultwords  = [word for word in querywords if word.lower() not in stopwords]\nresult = ' '.join(resultwords)\n\nprint(result)","metadata":{"id":"tByJ40d1cGtn","execution":{"iopub.status.busy":"2021-12-05T23:15:34.235726Z","iopub.execute_input":"2021-12-05T23:15:34.236256Z","iopub.status.idle":"2021-12-05T23:15:34.242219Z","shell.execute_reply.started":"2021-12-05T23:15:34.236215Z","shell.execute_reply":"2021-12-05T23:15:34.241419Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"tokenizer.word_index['cricket']","metadata":{"id":"RgnwXEtKNMG4","execution":{"iopub.status.busy":"2021-12-05T22:53:19.696990Z","iopub.execute_input":"2021-12-05T22:53:19.697753Z","iopub.status.idle":"2021-12-05T22:53:19.707168Z","shell.execute_reply.started":"2021-12-05T22:53:19.697692Z","shell.execute_reply":"2021-12-05T22:53:19.706280Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"\nimport json\n  \n  \nwith open('vocabulary.txt', 'w') as convert_file:\n     convert_file.write(json.dumps(tokenizer.word_index))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T22:53:56.847004Z","iopub.execute_input":"2021-12-05T22:53:56.847292Z","iopub.status.idle":"2021-12-05T22:53:56.859670Z","shell.execute_reply.started":"2021-12-05T22:53:56.847260Z","shell.execute_reply":"2021-12-05T22:53:56.858765Z"},"trusted":true},"execution_count":41,"outputs":[]}]}